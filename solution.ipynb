{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b1c872",
   "metadata": {},
   "source": [
    "# Проект: Нейросеть для автодополнения текстов\n",
    "\n",
    "Сравнение LSTM и distilgpt2 для задачи автодополнения коротких текстовых постов.\n",
    "\n",
    "**Датасет:** sentiment140 (короткие текстовые посты, ~1.6M записей)\n",
    "\n",
    "**Задача:** по началу текста (3/4) предсказать продолжение (1/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a0f564",
   "metadata": {},
   "source": [
    "## Этап 1. Сбор и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993632d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from data_utils import load_raw_data, preprocess_dataset, split_dataset\n",
    "\n",
    "df = load_raw_data('data/raw_dataset.txt')\n",
    "print(f'Исходный датасет: {len(df)} строк')\n",
    "\n",
    "df = preprocess_dataset(df)\n",
    "print(f'После очистки: {len(df)} строк')\n",
    "print(df['text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем и сохраняем\n",
    "train_df, val_df, test_df = split_dataset(df)\n",
    "df.to_csv('data/dataset_processed.csv', index=False)\n",
    "train_df.to_csv('data/train.csv', index=False)\n",
    "val_df.to_csv('data/val.csv', index=False)\n",
    "test_df.to_csv('data/test.csv', index=False)\n",
    "\n",
    "print(f'Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ad3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from next_token_dataset import create_dataloaders\n",
    "\n",
    "tokenizer, train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    'data/train.csv', 'data/val.csv', 'data/test.csv',\n",
    "    batch_size=128, min_freq=5\n",
    ")\n",
    "\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f'Batch X: {x_batch.shape}, Y: {y_batch.shape}')\n",
    "print(f'Словарь: {tokenizer.vocab_size} слов')\n",
    "\n",
    "# Пример кодирования/декодирования\n",
    "sample = 'i love this movie so much'\n",
    "encoded = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f'\\nОригинал: {sample}')\n",
    "print(f'Encoded:  {encoded}')\n",
    "print(f'Decoded:  {decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d834230",
   "metadata": {},
   "source": [
    "## Этап 2. Реализация LSTM модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lstm_model import LSTMModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model = LSTMModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "print(f'Параметры модели: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "# Проверка forward\n",
    "logits, _ = model(x_batch.to(device))\n",
    "print(f'Logits shape: {logits.shape}')\n",
    "\n",
    "# Проверка генерации (необученная модель)\n",
    "result = model.generate(tokenizer, 'i love this', max_new_tokens=5, device=device)\n",
    "print(f'\\nГенерация (до обучения): {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c8d660",
   "metadata": {},
   "source": [
    "## Этап 3. Обучение LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e31035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_train import train_model\n",
    "\n",
    "history = train_model(\n",
    "    model, train_loader, val_loader, tokenizer, device,\n",
    "    epochs=10, lr=0.001, save_path='models/lstm_best.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba756f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Графики обучения\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history['rouge1'])\n",
    "axes[1].set_title('ROUGE-1')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].plot(history['rouge2'])\n",
    "axes[2].set_title('ROUGE-2')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/training_history.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8dcb6",
   "metadata": {},
   "source": [
    "### Оценка LSTM на валидации и тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_train import compute_rouge\n",
    "\n",
    "model.load_state_dict(torch.load('models/lstm_best.pt', map_location=device))\n",
    "\n",
    "print('=== LSTM: Валидация ===')\n",
    "lstm_r1_val, lstm_r2_val, examples_val = compute_rouge(\n",
    "    model, val_loader, tokenizer, device, max_samples=500\n",
    ")\n",
    "print(f'ROUGE-1: {lstm_r1_val:.4f}')\n",
    "print(f'ROUGE-2: {lstm_r2_val:.4f}')\n",
    "print('\\nПримеры:')\n",
    "for ex in examples_val[:5]:\n",
    "    print(f'  Вход:   {ex[\"input\"]}')\n",
    "    print(f'  Таргет: {ex[\"target\"]}')\n",
    "    print(f'  Модель: {ex[\"generated\"]}\\n')\n",
    "\n",
    "print('\\n=== LSTM: Тест ===')\n",
    "lstm_r1_test, lstm_r2_test, examples_test = compute_rouge(\n",
    "    model, test_loader, tokenizer, device, max_samples=500\n",
    ")\n",
    "print(f'ROUGE-1: {lstm_r1_test:.4f}')\n",
    "print(f'ROUGE-2: {lstm_r2_test:.4f}')\n",
    "print('\\nПримеры:')\n",
    "for ex in examples_test[:5]:\n",
    "    print(f'  Вход:   {ex[\"input\"]}')\n",
    "    print(f'  Таргет: {ex[\"target\"]}')\n",
    "    print(f'  Модель: {ex[\"generated\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454fc03",
   "metadata": {},
   "source": [
    "### Примеры свободной генерации LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'i love this',\n",
    "    'going to the',\n",
    "    'i am so',\n",
    "    'just got back from',\n",
    "    'why is everyone',\n",
    "    'the weather is',\n",
    "    'i want to',\n",
    "    'happy birthday',\n",
    "]\n",
    "print('Примеры автодополнения (LSTM):\\n')\n",
    "for p in prompts:\n",
    "    result = model.generate(tokenizer, p, max_new_tokens=8, device=device)\n",
    "    print(f'  {p} -> {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6aaf85",
   "metadata": {},
   "source": [
    "## Этап 4. Предобученный трансформер (distilgpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca369fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_transformer_pipeline import evaluate_transformer\n",
    "\n",
    "print('=== distilgpt2: Валидация ===')\n",
    "gpt_r1_val, gpt_r2_val, gpt_ex_val = evaluate_transformer(\n",
    "    'data/val.csv', max_samples=500\n",
    ")\n",
    "print(f'ROUGE-1: {gpt_r1_val:.4f}')\n",
    "print(f'ROUGE-2: {gpt_r2_val:.4f}')\n",
    "print('\\nПримеры:')\n",
    "for ex in gpt_ex_val[:5]:\n",
    "    print(f'  Вход:   {ex[\"input\"]}')\n",
    "    print(f'  Таргет: {ex[\"target\"]}')\n",
    "    print(f'  Модель: {ex[\"generated\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa482e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== distilgpt2: Тест ===')\n",
    "gpt_r1_test, gpt_r2_test, gpt_ex_test = evaluate_transformer(\n",
    "    'data/test.csv', max_samples=500\n",
    ")\n",
    "print(f'ROUGE-1: {gpt_r1_test:.4f}')\n",
    "print(f'ROUGE-2: {gpt_r2_test:.4f}')\n",
    "print('\\nПримеры:')\n",
    "for ex in gpt_ex_test[:5]:\n",
    "    print(f'  Вход:   {ex[\"input\"]}')\n",
    "    print(f'  Таргет: {ex[\"target\"]}')\n",
    "    print(f'  Модель: {ex[\"generated\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc335",
   "metadata": {},
   "source": [
    "## Этап 5. Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b655a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сводная таблица метрик\n",
    "print('=' * 70)\n",
    "print(f'{\"Модель\":<15} {\"ROUGE-1 val\":>12} {\"ROUGE-2 val\":>12} {\"ROUGE-1 test\":>13} {\"ROUGE-2 test\":>13}')\n",
    "print('-' * 70)\n",
    "print(f'{\"LSTM\":<15} {lstm_r1_val:>12.4f} {lstm_r2_val:>12.4f} {lstm_r1_test:>13.4f} {lstm_r2_test:>13.4f}')\n",
    "print(f'{\"distilgpt2\":<15} {gpt_r1_val:>12.4f} {gpt_r2_val:>12.4f} {gpt_r1_test:>13.4f} {gpt_r2_test:>13.4f}')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d84cb6",
   "metadata": {},
   "source": [
    "### Анализ\n",
    "\n",
    "1. **distilgpt2 превосходит LSTM** по обеим метрикам ROUGE на валидационной и тестовой выборках.\n",
    "\n",
    "2. **Качество генерации**: По примерам видно, что distilgpt2 генерирует более грамматически корректные и семантически связные продолжения. LSTM генерирует условно связные тексты, но хуже попадает в контекст исходной фразы.\n",
    "\n",
    "3. **Причины разницы**:\n",
    "   - distilgpt2 предобучена на огромном корпусе текстов и уже «знает» английский язык.\n",
    "   - LSTM обучалась с нуля на датасете sentiment140.\n",
    "   - Архитектура трансформера лучше улавливает долгосрочные зависимости в тексте.\n",
    "\n",
    "4. **Размер моделей**:\n",
    "   - LSTM: ~15M параметров (embed_dim=64, hidden_dim=128, словарь ~77K слов).\n",
    "   - distilgpt2: ~82M параметров с subword-токенизацией (BPE), словарь ~50K подслов.\n",
    "\n",
    "### Рекомендации\n",
    "\n",
    "Для продуктового использования рекомендуется **distilgpt2**, так как:\n",
    "- Значительно лучшее качество генерации без необходимости обучения на собственных данных.\n",
    "- Приемлемый размер модели для мобильных устройств (с оптимизациями: квантизация, ONNX-конвертация).\n",
    "- Subword-токенизация делает модель устойчивой к опечаткам и новым словам.\n",
    "\n",
    "LSTM может быть оправдана только при жёстких ограничениях по памяти устройства (<10MB), но потребует значительно больше данных и вычислительных ресурсов для достижения приемлемого качества."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
